model:
  name: distilbert-base-uncased
  tokenizer: distilbert-base-uncased
processing:
  batch_size: 64
  max_length: 128
training:
  max_epochs: 2
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: ${training.limit_train_batches}
